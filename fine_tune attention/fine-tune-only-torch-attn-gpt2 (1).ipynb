{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\n\n# ----------------------------\n# Load model + tokenizer\n# ----------------------------\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Add pad token if missing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Freeze all params\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze attention QKV + projection in each transformer block\nfor layer in model.transformer.h:\n    for param in layer.attn.c_attn.parameters():\n        param.requires_grad = True\n    for param in layer.attn.c_proj.parameters():\n        param.requires_grad = True\n\n","metadata":{"_uuid":"abea7c9a-363b-4463-9971-45d236c36d6a","_cell_guid":"31b23bf0-771e-4cda-92c4-c13de103d4c6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-14T10:14:16.525670Z","iopub.execute_input":"2025-09-14T10:14:16.526328Z","iopub.status.idle":"2025-09-14T10:14:25.893744Z","shell.execute_reply.started":"2025-09-14T10:14:16.526268Z","shell.execute_reply":"2025-09-14T10:14:25.892852Z"}},"outputs":[{"name":"stderr","text":"2025-09-14 10:14:22.107691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757844862.130755     230 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757844862.137775     230 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ----------------------------\n# Dataset + tokenization\n# ----------------------------\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\ntokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].filter(\n    lambda x: x[\"text\"].strip() != \"\"\n)\ntokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].filter(\n    lambda x: x[\"text\"].strip() != \"\"\n)\n\ntokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].filter(\n    lambda x: x[\"text\"].strip() != \"\"\n)\n\n\n# Collator for LM tasks\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrain_loader = DataLoader(\n    tokenized_dataset[\"train\"].shuffle(seed=42).select(range(500)),\n    batch_size=2,\n    shuffle=True,\n    collate_fn=data_collator,\n)\n\nval_loader = DataLoader(\n    tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(100)),\n    batch_size=2,\n    shuffle=False,\n    collate_fn=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T10:15:17.973962Z","iopub.execute_input":"2025-09-14T10:15:17.974558Z","iopub.status.idle":"2025-09-14T10:15:22.058071Z","shell.execute_reply.started":"2025-09-14T10:15:17.974529Z","shell.execute_reply":"2025-09-14T10:15:22.057364Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506a0a7fabdd49169cb9a3414faab3b6"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\n# ----------------------------\n# Training setup\n# ----------------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6)\n\nnum_epochs = 10\nlog_interval = 20\n\n# ----------------------------\n# Training loop\n# ----------------------------\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if step % log_interval == 0 and step > 0:\n            print(f\"Epoch {epoch+1} | Step {step} | Loss {loss.item():.4f}\")\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Avg Train Loss {avg_loss:.4f}\")\n\n    # ------------------------\n    # Validation\n    # ------------------------\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | Validation Loss {val_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T10:16:02.473933Z","iopub.execute_input":"2025-09-14T10:16:02.474252Z","iopub.status.idle":"2025-09-14T10:18:48.751490Z","shell.execute_reply.started":"2025-09-14T10:16:02.474222Z","shell.execute_reply":"2025-09-14T10:18:48.750788Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Step 20 | Loss 4.1175\nEpoch 1 | Step 40 | Loss 4.6899\nEpoch 1 | Step 60 | Loss 3.7422\nEpoch 1 | Step 80 | Loss 3.8952\nEpoch 1 | Step 100 | Loss 4.4378\nEpoch 1 | Step 120 | Loss 4.1841\nEpoch 1 | Step 140 | Loss 4.7822\nEpoch 1 | Step 160 | Loss 3.8671\nEpoch 1 | Step 180 | Loss 4.4331\nEpoch 1 | Step 200 | Loss 4.0598\nEpoch 1 | Step 220 | Loss 4.0144\nEpoch 1 | Step 240 | Loss 4.3399\nEpoch 1 | Avg Train Loss 4.4187\nEpoch 1 | Validation Loss 4.0927\nEpoch 2 | Step 20 | Loss 3.1978\nEpoch 2 | Step 40 | Loss 4.4394\nEpoch 2 | Step 60 | Loss 6.5381\nEpoch 2 | Step 80 | Loss 4.6796\nEpoch 2 | Step 100 | Loss 4.1046\nEpoch 2 | Step 120 | Loss 4.2617\nEpoch 2 | Step 140 | Loss 4.0186\nEpoch 2 | Step 160 | Loss 4.2912\nEpoch 2 | Step 180 | Loss 5.2949\nEpoch 2 | Step 200 | Loss 3.6905\nEpoch 2 | Step 220 | Loss 4.2822\nEpoch 2 | Step 240 | Loss 4.0733\nEpoch 2 | Avg Train Loss 4.1680\nEpoch 2 | Validation Loss 3.8921\nEpoch 3 | Step 20 | Loss 6.1193\nEpoch 3 | Step 40 | Loss 5.5229\nEpoch 3 | Step 60 | Loss 4.2124\nEpoch 3 | Step 80 | Loss 4.5911\nEpoch 3 | Step 100 | Loss 4.1061\nEpoch 3 | Step 120 | Loss 4.2097\nEpoch 3 | Step 140 | Loss 4.5132\nEpoch 3 | Step 160 | Loss 4.0997\nEpoch 3 | Step 180 | Loss 4.4242\nEpoch 3 | Step 200 | Loss 4.4562\nEpoch 3 | Step 220 | Loss 2.9577\nEpoch 3 | Step 240 | Loss 3.7286\nEpoch 3 | Avg Train Loss 4.0505\nEpoch 3 | Validation Loss 3.7971\nEpoch 4 | Step 20 | Loss 4.0811\nEpoch 4 | Step 40 | Loss 3.3625\nEpoch 4 | Step 60 | Loss 3.4575\nEpoch 4 | Step 80 | Loss 4.0671\nEpoch 4 | Step 100 | Loss 3.6330\nEpoch 4 | Step 120 | Loss 3.7985\nEpoch 4 | Step 140 | Loss 3.5340\nEpoch 4 | Step 160 | Loss 3.5285\nEpoch 4 | Step 180 | Loss 3.9476\nEpoch 4 | Step 200 | Loss 4.3258\nEpoch 4 | Step 220 | Loss 3.9672\nEpoch 4 | Step 240 | Loss 4.0875\nEpoch 4 | Avg Train Loss 3.9228\nEpoch 4 | Validation Loss 3.7399\nEpoch 5 | Step 20 | Loss 3.1709\nEpoch 5 | Step 40 | Loss 4.4350\nEpoch 5 | Step 60 | Loss 3.6651\nEpoch 5 | Step 80 | Loss 4.1143\nEpoch 5 | Step 100 | Loss 3.8720\nEpoch 5 | Step 120 | Loss 4.6127\nEpoch 5 | Step 140 | Loss 3.4451\nEpoch 5 | Step 160 | Loss 4.4010\nEpoch 5 | Step 180 | Loss 3.7037\nEpoch 5 | Step 200 | Loss 4.2796\nEpoch 5 | Step 220 | Loss 3.5845\nEpoch 5 | Step 240 | Loss 3.7590\nEpoch 5 | Avg Train Loss 3.9119\nEpoch 5 | Validation Loss 3.6964\nEpoch 6 | Step 20 | Loss 4.1667\nEpoch 6 | Step 40 | Loss 3.7390\nEpoch 6 | Step 60 | Loss 4.1797\nEpoch 6 | Step 80 | Loss 3.4933\nEpoch 6 | Step 100 | Loss 3.7656\nEpoch 6 | Step 120 | Loss 4.2692\nEpoch 6 | Step 140 | Loss 3.7840\nEpoch 6 | Step 160 | Loss 4.3013\nEpoch 6 | Step 180 | Loss 4.6228\nEpoch 6 | Step 200 | Loss 4.1513\nEpoch 6 | Step 220 | Loss 3.5218\nEpoch 6 | Step 240 | Loss 4.3658\nEpoch 6 | Avg Train Loss 3.8107\nEpoch 6 | Validation Loss 3.6705\nEpoch 7 | Step 20 | Loss 4.0470\nEpoch 7 | Step 40 | Loss 4.0845\nEpoch 7 | Step 60 | Loss 3.6887\nEpoch 7 | Step 80 | Loss 4.0692\nEpoch 7 | Step 100 | Loss 4.0328\nEpoch 7 | Step 120 | Loss 3.7379\nEpoch 7 | Step 140 | Loss 3.0420\nEpoch 7 | Step 160 | Loss 4.4795\nEpoch 7 | Step 180 | Loss 3.4753\nEpoch 7 | Step 200 | Loss 4.0782\nEpoch 7 | Step 220 | Loss 3.5286\nEpoch 7 | Step 240 | Loss 3.3905\nEpoch 7 | Avg Train Loss 3.7754\nEpoch 7 | Validation Loss 3.6370\nEpoch 8 | Step 20 | Loss 3.4459\nEpoch 8 | Step 40 | Loss 3.6359\nEpoch 8 | Step 60 | Loss 3.9701\nEpoch 8 | Step 80 | Loss 3.5957\nEpoch 8 | Step 100 | Loss 2.7217\nEpoch 8 | Step 120 | Loss 3.7108\nEpoch 8 | Step 140 | Loss 3.7924\nEpoch 8 | Step 160 | Loss 3.9877\nEpoch 8 | Step 180 | Loss 4.1167\nEpoch 8 | Step 200 | Loss 4.0992\nEpoch 8 | Step 220 | Loss 2.4319\nEpoch 8 | Step 240 | Loss 3.7011\nEpoch 8 | Avg Train Loss 3.7484\nEpoch 8 | Validation Loss 3.6216\nEpoch 9 | Step 20 | Loss 3.7511\nEpoch 9 | Step 40 | Loss 3.4000\nEpoch 9 | Step 60 | Loss 3.6252\nEpoch 9 | Step 80 | Loss 2.9501\nEpoch 9 | Step 100 | Loss 4.0636\nEpoch 9 | Step 120 | Loss 4.2708\nEpoch 9 | Step 140 | Loss 3.8661\nEpoch 9 | Step 160 | Loss 3.2629\nEpoch 9 | Step 180 | Loss 4.3226\nEpoch 9 | Step 200 | Loss 3.3400\nEpoch 9 | Step 220 | Loss 4.0581\nEpoch 9 | Step 240 | Loss 3.4997\nEpoch 9 | Avg Train Loss 3.7386\nEpoch 9 | Validation Loss 3.5940\nEpoch 10 | Step 20 | Loss 3.3122\nEpoch 10 | Step 40 | Loss 2.9199\nEpoch 10 | Step 60 | Loss 4.0811\nEpoch 10 | Step 80 | Loss 3.5852\nEpoch 10 | Step 100 | Loss 3.4230\nEpoch 10 | Step 120 | Loss 4.0056\nEpoch 10 | Step 140 | Loss 3.9687\nEpoch 10 | Step 160 | Loss 3.7886\nEpoch 10 | Step 180 | Loss 3.5246\nEpoch 10 | Step 200 | Loss 3.6406\nEpoch 10 | Step 220 | Loss 3.2614\nEpoch 10 | Step 240 | Loss 4.0212\nEpoch 10 | Avg Train Loss 3.6836\nEpoch 10 | Validation Loss 3.5798\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ----------------------------\n# Save fine-tuned model\n# ----------------------------\nsave_dir = \"./gpt2_finetuned_attention\"\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T10:21:10.914099Z","iopub.execute_input":"2025-09-14T10:21:10.914429Z","iopub.status.idle":"2025-09-14T10:21:12.121170Z","shell.execute_reply.started":"2025-09-14T10:21:10.914409Z","shell.execute_reply":"2025-09-14T10:21:12.120409Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('./gpt2_finetuned_attention/tokenizer_config.json',\n './gpt2_finetuned_attention/special_tokens_map.json',\n './gpt2_finetuned_attention/vocab.json',\n './gpt2_finetuned_attention/merges.txt',\n './gpt2_finetuned_attention/added_tokens.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"\n# ----------------------------\n# Inference\n# ----------------------------\nprompt = \"Artificial intelligence in 2025\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_length=50,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.8\n    )\n\nprint(\"Generated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T10:21:16.607786Z","iopub.execute_input":"2025-09-14T10:21:16.608058Z","iopub.status.idle":"2025-09-14T10:21:17.674543Z","shell.execute_reply.started":"2025-09-14T10:21:16.608040Z","shell.execute_reply":"2025-09-14T10:21:17.673644Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text:\n Artificial intelligence in 2025\n\nIn order to solve the problem of artificial intelligence in 2025 , there are two main approaches that can be used:\n\n1) Develop a system that can detect human actions and predict future actions\n\n2) Use\n","output_type":"stream"}],"execution_count":9}]}